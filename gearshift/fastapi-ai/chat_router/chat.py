from fastapi import APIRouter
from pydantic import BaseModel
from . import extractor, rag, terms
from langchain_core.messages import HumanMessage
from .agent_chain import agent_executor
import logging

logger = logging.getLogger("chat_logger")
logging.basicConfig(level=logging.INFO)

router = APIRouter(
    prefix="/chat",
    tags=["chat"],
)

class UserQuery(BaseModel):
    message: str


### ì‹œì„¸ ###
class ParsedCarInfo(BaseModel):
    model: str
    fuel_type: str
    offered_price: int

@router.post("/price")
def chat(query: UserQuery):
    parsed = extractor.extract_info_with_llm(query.message)
    answer = rag.get_rag_response(parsed)
    return {
        "parsed_info": parsed,
        "answer": answer
    }


### ì•½ê´€ ###
@router.post("/term")
async def ask_question(query: UserQuery):
    query = query.message
    context = terms.retriever.invoke(query)
    context_docs = context[0].page_content if context else ""
    result = terms.chain.invoke({"question": query, "context": context_docs})
    return {"answer": result}


### í†µí•© ###
@router.post("/integrate")
async def integrated_chat(query: UserQuery):
    result = await agent_executor.ainvoke({"messages": [HumanMessage(content=query.message)]})

    # LLMì´ toolì„ ì“°ì§€ ì•Šê³  LLMì˜ ìƒê°ìœ¼ë¡œ ë‹µë³€í–ˆëŠ”ì§€ ì²´í¬
    if isinstance(result, str):
        logger.error('LLMì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë§ˆìŒëŒ€ë¡œ ë‹µë³€')
        return {"answer": "ì£„ì†¡í•´ìš”, í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ì—†ì–´ìš” ğŸ˜¢"}

    output_text = result["output"]
    return {"answer": output_text}