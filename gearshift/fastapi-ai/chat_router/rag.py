import pandas as pd
from langchain_upstage import ChatUpstage
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from . import settings

# LLM ì´ˆê¸°í™”
llm = ChatUpstage(api_key=settings.LLM_API_KEY, model="solar-1-mini-chat")
output_parser = StrOutputParser()

# ìì—°ì–´ ì‘ë‹µ í…œí”Œë¦¿
response_prompt = PromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì¤‘ê³ ì°¨ ì‹œì„¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

{filtered_info} ì°¨ëŸ‰ì„ {offered_price}ë§Œì›ì— êµ¬ë§¤í•˜ë ¤ê³  í•©ë‹ˆë‹¤.
ì´ ì°¨ëŸ‰ì˜ í‰ê·  ì‹œì„¸ëŠ” {avg_price}ë§Œì›ì…ë‹ˆë‹¤.
{verdict}
ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
ì¤‘ë³µë˜ê±°ë‚˜ ë»”í•œ í‘œí˜„ì€ í”¼í•˜ê³ , í•µì‹¬ ì •ë³´ë§Œ ê°„ê²°í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”.
ë˜í•œ "ì‹ ì¤‘íˆ ê²°ì •í•˜ì„¸ìš”", "ì „ë¬¸ê°€ì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”", "ë¬¸ì˜í•´ ì£¼ì„¸ìš”" ê°™ì€ ë§ì€ í•˜ì§€ ë§ˆì„¸ìš”.
""")

def get_rag_response(info):
    # 1. ë°ì´í„° ë¡œë“œ ë° ì •ë¦¬
    df = pd.read_csv(settings.CSV_PATH)
    df = df.rename(columns={'ëª¨ë¸ëª…': 'model', 'ì—°ë£Œ': 'fuel_type', 'íŠ¸ë¦¼': 'trim', 'ê°€ê²©': 'price'})

    # ğŸ”¥ ê³µë°± ì œê±° í•„ìˆ˜! (ì´ ì¤„ ì¶”ê°€)
    df['model'] = df['model'].astype(str).str.strip()
    df['fuel_type'] = df['fuel_type'].astype(str).str.strip()
    df['trim'] = df['trim'].astype(str).str.strip()

    # 2. ì…ë ¥ ì •ë³´ ì •ë¦¬
    model = info.get('model', '').strip()
    fuel = info.get('fuel_type', '').strip()
    trim = info.get('trim', '').strip() if 'trim' in info else ''
    offer = float(info.get('offered_price'))

    # 3. ì¡°ê±´ë³„ í•„í„°ë§
    filtered = pd.DataFrame()

    # ìš°ì„ ìˆœìœ„ 1: ëª¨ë¸ + ì—°ë£Œ + íŠ¸ë¦¼
    if model and fuel and trim:
        filtered = df[
            (df['model'] == model) &
            (df['fuel_type'] == fuel) &
            (df['trim'] == trim)
            ]

    # ìš°ì„ ìˆœìœ„ 2: ëª¨ë¸ + íŠ¸ë¦¼
    if filtered.empty and model and trim:
        filtered = df[
            (df['model'] == model) &
            (df['trim'] == trim)
            ]

    # 4. ê²°ê³¼ ì—†ìœ¼ë©´ ì‚¬ê³¼ ë©”ì‹œì§€
    if filtered.empty:
        return "ì£„ì†¡í•˜ì§€ë§Œ ì œê³µëœ ì •ë³´ë¡œëŠ” ì •í™•í•œ íŒë‹¨ì´ ì–´ë µìŠµë‹ˆë‹¤."

    # 5. í‰ê·  ì‹œì„¸ ê³„ì‚°
    avg_price = filtered['price'].astype(float).mean()

    # 6. ê°€ê²© ë¹„êµ íŒë‹¨
    if offer < avg_price:
        verdict = "ì œì‹œí•œ ê°€ê²©ì´ ì‹œì„¸ë³´ë‹¤ ì €ë ´í•©ë‹ˆë‹¤. ì¢‹ì€ ì¡°ê±´ì´ì—ìš”!"
    elif offer > avg_price:
        verdict = "ì œì‹œí•œ ê°€ê²©ì´ ì‹œì„¸ë³´ë‹¤ ë¹„ìŒ‰ë‹ˆë‹¤. ì‹ ì¤‘íˆ ê³ ë ¤í•´ë³´ì„¸ìš”."
    else:
        verdict = "ì œì‹œí•œ ê°€ê²©ì€ ì‹œì„¸ì™€ ê±°ì˜ ê°™ìŠµë‹ˆë‹¤."

    # 7. ì°¨ëŸ‰ ì •ë³´ ì¡°í•©
    filtered_info = f"{model}"
    if fuel:
        filtered_info += f" ({fuel})"
    if trim:
        filtered_info += f" [{trim}]"

    # 8. í”„ë¡¬í”„íŠ¸ì— ì ìš© í›„ LLM ì‘ë‹µ
    prompt = response_prompt.format(
        filtered_info=filtered_info,
        offered_price=offer,
        avg_price=int(avg_price),
        verdict=verdict
    )

    return (llm | output_parser).invoke(prompt)
